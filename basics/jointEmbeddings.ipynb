{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cf06bcd-5fea-492c-ba22-a5f4a2907af8",
   "metadata": {},
   "source": [
    "In this example, we first load the pre-trained CLIP model (\"ViT-B/32\" in this case) and the necessary preprocessing functions. We then load an example image and caption, and generate embeddings for each using the encode_image and encode_text functions provided by the CLIP model. Finally, we concatenate the two embeddings to create a joint embedding of size 1024.\n",
    "\n",
    "Note that this example only uses one image and one caption, but the same approach can be used to create joint embeddings for larger datasets by iterating over the images and captions and concatenating their embeddings. Additionally, the pre-trained CLIP model used in this example can be fine-tuned on a larger image-caption dataset to learn a joint embedding space that is better suited to a specific task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91df4058-52a1-4c0a-8a22-29573249f6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "# Load a pre-trained ResNet-50 model for image feature extraction\n",
    "resnet = models.resnet50(pretrained=True)\n",
    "modules = list(resnet.children())[:-1]\n",
    "resnet = nn.Sequential(*modules)\n",
    "resnet.eval()\n",
    "\n",
    "# Load a pre-trained BERT model for text feature extraction\n",
    "bert = torch.hub.load('huggingface/pytorch-transformers', 'model', 'bert-base-uncased')\n",
    "bert.eval()\n",
    "\n",
    "# Define the linear projection for mapping image and text embeddings to joint space\n",
    "projection = nn.Linear(2048, 512)  # Map ResNet-50 output (2048) to 512-dim joint space\n",
    "projection.eval()\n",
    "\n",
    "# Define an image and a text input\n",
    "image_path = 'path/to/image.jpg'\n",
    "text_input = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "# Extract image embedding using ResNet-50\n",
    "image = Image.open(image_path)\n",
    "image_tensor = transforms.ToTensor()(image)\n",
    "image_tensor = image_tensor.unsqueeze_(0)\n",
    "with torch.no_grad():\n",
    "    image_embedding = resnet(image_tensor).squeeze()\n",
    "\n",
    "# Extract text embedding using BERT\n",
    "input_ids = torch.tensor(bert.tokenizer.encode(text_input, add_special_tokens=True)).unsqueeze(0)\n",
    "with torch.no_grad():\n",
    "    bert_outputs = bert(input_ids)\n",
    "    text_embedding = bert_outputs[0][:, 0, :]\n",
    "\n",
    "# Map image and text embeddings to joint space\n",
    "with torch.no_grad():\n",
    "    image_embedding = projection(image_embedding)\n",
    "    text_embedding = projection(text_embedding)\n",
    "\n",
    "# Compute cosine similarity between image and text embeddings in joint space\n",
    "cos_sim = nn.CosineSimilarity(dim=0)\n",
    "similarity = cos_sim(image_embedding, text_embedding)\n",
    "\n",
    "print(similarity)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
