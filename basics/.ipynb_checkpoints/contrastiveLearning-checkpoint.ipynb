{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9a54c66-69c0-4a1e-8a0a-2b1885dce203",
   "metadata": {},
   "source": [
    "This code defines a PyTorch module called ImageTextContrastiveLoss that computes the contrastive loss between image and text embeddings. The forward method takes in three arguments: image_embeddings and text_embeddings, which are the embeddings for the images and captions, respectively, and labels, which is a binary tensor indicating whether each image-caption pair is semantically related or unrelated.\n",
    "\n",
    "The first step in the forward method is to compute the pairwise cosine similarity between all image-caption pairs using the cosine_similarity function from PyTorch's functional module. A mask is then created to remove self-similarities, as these would result in the model learning to map each image and caption to itself rather than learning meaningful embeddings.\n",
    "\n",
    "Next, the contrastive loss is computed using the similarities and labels. The contrastive loss encourages the model to learn embeddings that are close together for semantically related image-caption pairs and far apart for semantically unrelated pairs. The loss is computed as a combination of two terms: a positive term that penalizes the similarity between semantically unrelated pairs and a negative term that encourages the similarity between semantically related pairs. The margin hyperparameter controls the distance between these two terms.\n",
    "\n",
    "Finally, the loss is averaged over all image-caption pairs in the batch and returned. This loss can be used to fine-tune a pre-trained image-caption model such as CLIP on a specific image-caption dataset, to learn a joint embedding space that is better suited to a specific task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd108c24-603e-4e57-b0f5-ee87fd1c9496",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ImageTextContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin=0.2):\n",
    "        super(ImageTextContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "        \n",
    "    def forward(self, image_embeddings, text_embeddings, labels):\n",
    "        # Compute the pairwise cosine similarity matrix\n",
    "        similarities = F.cosine_similarity(image_embeddings.unsqueeze(1), text_embeddings.unsqueeze(0), dim=-1)\n",
    "        \n",
    "        # Create a mask to remove self-similarities\n",
    "        \"\"\"\n",
    "        When training a model using contrastive learning, the objective is to learn\n",
    "        representations that can distinguish between similar and dissimilar pairs of\n",
    "        samples. Typically, we form positive pairs by selecting two augmented versions\n",
    "        of the same sample, and negative pairs by selecting one sample from a differen\n",
    "        class or from a different batch. The contrastive loss function encourages the\n",
    "        model to push positive pairs closer together and negative pairs farther apart\n",
    "        in the embedding space.\n",
    "\n",
    "        However, when computing the contrastive loss, we want to exclude the possibility\n",
    "        of a sample being paired with itself. This is because the model can trivially\n",
    "        achieve a low loss by simply mapping each sample to its own point in the embedding\n",
    "        space, which does not provide useful information for downstream tasks.\n",
    "        \"\"\"\n",
    "        mask = torch.eye(similarities.shape[0], dtype=torch.bool).to(image_embeddings.device)\n",
    "        similarities = similarities[~mask].view(similarities.shape[0], -1)\n",
    "        \n",
    "        # Compute the contrastive loss\n",
    "        loss = 0.5 * (1 - labels.float()) * torch.pow(similarities, 2) + 0.5 * labels.float() *\\\n",
    "            torch.pow(torch.clamp(self.margin - similarities, min=0), 2)\n",
    "        loss = loss.mean()\n",
    "        \n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3701de3-9002-4ce7-ba02-c310af821920",
   "metadata": {},
   "source": [
    "CLIP does not directly create augmented versions of images or texts. Instead, it expects the user to provide the augmented versions as inputs during training. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766b210b-4b38-4eb6-a236-f59c142d0d7b",
   "metadata": {},
   "source": [
    "Here's an example of how you can use PyTorch's transforms module to apply image augmentations and text augumentations:\n",
    "\n",
    "For text inputs, you can use techniques such as random deletion, random swapping of words, and random insertion of words to create augmented versions of a text. These techniques can help improve the model's ability to handle noise and variations in the text inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84e632a-0299-4dd5-a491-e09f8bde6bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"for images\"\"\"\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Define a list of transforms\n",
    "transform_list = [\n",
    "    transforms.RandomCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "]\n",
    "\n",
    "# Create a composed transform\n",
    "image_transforms = transforms.Compose(transform_list)\n",
    "\n",
    "# Load an image and apply the transforms\n",
    "image = Image.open('image.jpg')\n",
    "image_augmented = image_transforms(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d9a6d2-fc32-4669-84a9-cb7e676eb022",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"for text\"\"\"\n",
    "!pip install nlpaug\n",
    "\n",
    "import nlpaug.augmenter.word as naw\n",
    "\n",
    "# Define a text to augment\n",
    "text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "# Define an augmentation technique\n",
    "aug = naw.RandomWordAug(action='swap')\n",
    "\n",
    "# Apply the augmentation\n",
    "text_augmented = aug.augment(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
