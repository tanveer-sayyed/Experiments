{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39486900-7165-4de9-84b6-e78e92e9b8e9",
   "metadata": {},
   "source": [
    "### A dibiased model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8842c5fc-0a06-4c55-95b4-c9f5d755a2f2",
   "metadata": {},
   "source": [
    "Then their task was to try to determine, for words that differ on this\n",
    "gender dimension, whether that gender difference is appropriate or\n",
    "inappropriate. Let’s say king and queen are appropriately separated by\n",
    "gender, and ditto father and mother , but maybe we don’t want to regard—\n",
    "as word2vec by default does— Home Depot as the gender-flipped version of\n",
    "JC Penney ; or alcoholism and eating disorders ; or pilot and flight\n",
    "attendant.\n",
    "\n",
    "- How, then, to tease apart the problematic from the unproblematic gender associations for not just a handful but for hundreds of thousands of different words?\n",
    "- How to know which analogies should be kept, which should be adjusted, and which should be purged entirely?\n",
    "\n",
    "The team did their best, identifying 218 such gender-specific\n",
    "words in a subset of their model’s dictionary, and let their system\n",
    "**extrapolate** to the rest of the dictionary. For all words outside this set, they set the gender component of the word’s representation to zero. Then they adjusted the\n",
    "representations of all the gender-related words such that pairs of equivalent\n",
    "terms—say, “brother” and “sister”—were “centered” around this zero\n",
    "point. Said another way, they were adjusted so that neither term was\n",
    "represented in the model as more “gender-specific” or more “gender-\n",
    "neutral” than the other.\n",
    "\n",
    "This neutralization came at a small cost—the model now, for instance,\n",
    "thought it just as likely that someone could be “grandmothered in” as\n",
    "“grandfathered in” to a legal exemption. But maybe this was a price worth\n",
    "paying—and you could always decide how much prediction error you were\n",
    "willing to trade for how much debiasing and set an appropriate tradeoff.\n",
    "\n",
    "But later research proved that this debiasing was just - “lipstick on a pig.” Implicit connection among these stereotypically “feminine” professions themselves—between “nurse” and “receptionist”—remained.  In fact, such only partial debiasing may actually\n",
    "make the problem worse, they argue, in the sense that it leaves the majority\n",
    "of these stereotypical associations intact while removing the ones that are\n",
    "the most visible and easiest to measure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e234ab-2d9c-429e-b13a-2d34cd46d5fe",
   "metadata": {},
   "source": [
    "A classic test of unconscious bias in humans used in the social sciences is the “implicit association test,” . A team of computer scientists at Princeton—postdoc Aylin Caliskan and professors Joanna Bryson and Arvind Narayanan—found that the distance between embeddings in word2vec and other widely used word-embedding models uncannily mirrors this human reaction-time data. The slower people are to identify any two groups of words, the farther away those word vectors were in the model. The model’s biases, in other words, are, for better or worse, very much our own."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65412866-6df8-4990-b09a-a53eebe86a45",
   "metadata": {},
   "source": [
    "### The model’s biases, in other words, are, for better or worse, very much our own."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff69dcf5-464c-4dfd-9b6d-6e51d654c280",
   "metadata": {},
   "source": [
    "The fact that the embeddings that emerge from this “magical”\n",
    "optimization process are so uncannily and discomfitingly useful as a mirror\n",
    "for society means that we have, in effect, added a diagnostic tool to the\n",
    "arsenal of social science. We can use these embeddings to quantify\n",
    "something in precise detail about society at a given snapshot in time. And\n",
    "regardless of causation—whether it’s changes in the objective reality thatchange the way we speak, or vice versa, or whether both are driven by some\n",
    "deeper cause—we can use these snapshots to watch society change."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bb63a9-be0f-4fbb-b1be-1d787bd54982",
   "metadata": {},
   "source": [
    "- Computer scientists are reaching out to the social sciences as they begin to think more broadly about what goes into the models they build. Likewise, social scientists are reaching out to the machine-learning community and are finding they now have a powerful new microscope at their disposal.\n",
    "- Second is that biases and connotations are real. They are now measurable, in detail and with precision and are dynamic - the story of our language is the story of our culture.\n",
    "- Third: These models should absolutely be used with caution.\n",
    "- Fourthly time. As Princeton’s Arvind Narayanan puts it: “Contrary to the ‘tech moves too fast for society to keep up’ cliché, commercial deployments of tech often move glacially—just look at the banking and airline mainframes still running. ML [machine-learning] models being trained today might still be in production in 50 years, and that’s terrifying.”\n",
    "- Assuming the model will not change the world is false. Indeed, uncareful deployment of these models might produce a feedback loop from which recovery becomes ever more difficult or requires ever greater interventions.\n",
    "- Lastly, these models offer us a digital sextant as we look ahead as a society. They **must** be used descriptively rather than prescriptively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f1adf3-a4b8-4509-8dca-f051f525bf1a",
   "metadata": {},
   "source": [
    "#### intractability: not easily managed or cured (a theoritical study)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae36277-4990-41e7-a2d4-ff039c7e3d9b",
   "metadata": {},
   "source": [
    "What they and their colleagues began to find was that not only were\n",
    "there enormous complexities in translating our philosophical and legal ideas\n",
    "about fairness into hard mathematical constraints but, in fact, much of the\n",
    "leading thought and practice, some of it decades old, was deeply misguided\n",
    "—and had the potential to be downright harmful.\n",
    "\n",
    "If we hear in the press that a model “uses race” (or gender, etc.) as an attribute, we are led to believe something has already gone deeply wrong. Simply removing the “protected attribute” is insufficient. As long as the model takes in features that are correlated with, say, gender or race, avoiding explicitly mentioning it will do little good. This is known as the concept of “redundant encodings.” The gender attribute is redundantly encoded across other variables. eg. number of previous convictions, can become a redundant encoding of race. In fact, one of the perverse upshots of redundant\n",
    "encodings is that being blind to these attributes may make things worse.\n",
    "\n",
    "\n",
    "For instance, a machine-learning model used in a recruiting context might penalize a candidate for not having had a job in the prior year. We might not want this penalty applied to pregnant women or recent mothers, however—but this will be difficult if the model must be “gender-blind” and can’t include gender itself, nor something so strongly\n",
    "connected to it as pregnancy. Thus. omitting the redundant encoding also makes it impossible not only to measure this bias but also to mitigate it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1833a34-fa11-4166-b568-969617406ea2",
   "metadata": {},
   "source": [
    "#### fairness through blindness doesn’t work!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ad4608-0654-4830-8241-0b672ca20b2b",
   "metadata": {},
   "source": [
    "If an algorithms decision is \"protected\" by law then it will lead presumtively to legal harm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87dcc276-6928-4f52-a886-d05fffbd6bb8",
   "metadata": {},
   "source": [
    "A tool that is calibrated, she writes, “cannot have equal false positive and negative rates across groups, when the recidivism prevalence differs across those groups.” The impossibility proofs also show that equalizing the false positive and false negative rates means giving up on calibration. Then it will be unclear what it means to have a risk score. However, even those who emphasize the importance of calibration think\n",
    "that it alone isn’t enough. As Corbett-Davies says, “Calibration, though generally desirable, provides little guarantee that decisions are equitable.”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdf711b-eb42-46de-b52f-5832116a61dd",
   "metadata": {},
   "source": [
    "Because this data is collected as a by-product of police activity,\n",
    "predictions made on the basis of patterns learned from this data do not\n",
    "pertain to future instances of crime on the whole. They pertain to future\n",
    "instances of crime that becomes known to police. In this sense, predictive\n",
    "policing is aptly named: it is predicting future policing, not future crime.\n",
    "\n",
    "- What happens if a parolee is not caught at crime?\n",
    "- What happens if over policed and high risk individuals become ground truth?\n",
    "\n",
    "This is particularly worrisome in the context of predictive policing,\n",
    "where this training data is used to determine the very police activity that, in\n",
    "turn, generates arrest data—setting up a potential long-term feedback\n",
    "loop. The model becomes increasingly confident\n",
    "that the locations most likely to experience further criminal activity are\n",
    "exactly the locations they had previously believed to be high in crime:\n",
    "selection bias meets confirmation bias. This feedback loop, in turn, further biases\n",
    "its training data.\n",
    "\n",
    "At a minimum, it seems clear that we should\n",
    "know exactly what it is that our predictive tools are designed to predict—\n",
    "and we should be very cautious about using them outside of those\n",
    "parameters. “USE ONLY AS DIRECTED ,”\n",
    "\n",
    "Do that better predictions lead to better public safety? However, “improvements in the accuracy of predictions alone may not result in a reduction in crime . . . perhaps more importantly, law\n",
    "enforcement needs better information about what to do with the predictions”. Predictions are not an end in themselves. What is better: a world in which we can be 99% sure where a crime will occur and when, or a world in which there is simply 99% less crime? Are we missing something larger?\n",
    "\n",
    "“So this brings me to my main point,” Moritz Hardt tells me. A\n",
    "machine-learning model, trained by data, “is by definition a tool to predict\n",
    "the future, given that it looks like the past. . . . That’s why it’s\n",
    "fundamentally the wrong tool for a lot of domains, where you’re trying to\n",
    "design interventions and mechanisms to change the world.” Prediction offers a bit of a dystopian perspective on the topic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b86232c-7ca6-46b1-b289-f3d5b36fe6c8",
   "metadata": {},
   "source": [
    "# Transparency\n",
    "\n",
    "It’s often observed in the field that the most powerful models are on the\n",
    "whole the least intelligible, and the most intelligible are among the least\n",
    "accurate. Neural nets are really good, they’re\n",
    "accurate; but they’re completely opaque and unintelligible, and I think\n",
    "that’s dangerous now. \n",
    "\n",
    "So we need models that offer best of both worlds.\n",
    "\n",
    "For one, they seem to suggest that, whatever\n",
    "myriad issues we face in turning decision-making over to statistical models,\n",
    "human judgment alone is not a viable alternative. At the same time, perhaps\n",
    "complex, elaborate models really aren’t necessary to match or exceed this\n",
    "human baseline.\n",
    "A tantalizing question lurks, however: Namely, what explains this\n",
    "surprising verdict? Is human judgment really that bad? Are simple linear\n",
    "models of a handful of variables really that good? Or . . . a third possibility:\n",
    "Has human expertise somehow managed to enter into the simple models\n",
    "where we least expect it? Were we looking for it in the wrong place?\n",
    "\n",
    "It’s an exciting time for researchers working on this set of questions.\n",
    "Simple models are amazingly competitive—and then some—with human\n",
    "expertise. Modern techniques give us ways of deriving ideal simple models.\n",
    "With that said, there are cases where complexity is simply unavoidable;\n",
    "the obvious one is models that don’t have the benefit of human experts\n",
    "filtering their inputs to meaningful quantities of manageable size. Some\n",
    "models must, for better or worse, deal not with human abstractions like\n",
    "“GRE score” and “number of prior offenses” but with raw linguistic, audio,\n",
    "or visual data. Some medical diagnostic tools can be fed human inputs, like\n",
    "“mild fever” and “asthmatic,” while others might be shown an X-ray or\n",
    "CAT scan directly and must make some sense of it. A self-driving car, of\n",
    "course, must deal with a stream of radar, lidar, and visual data directly. In\n",
    "such cases we have little choice but the kinds of large, multimillion-\n",
    "parameter “black box” neural networks that have such a reputation for\n",
    "inscrutability. But we are not without resources here as well, on the science\n",
    "of transparency’s other, wilder frontier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61b9eeb-319e-4539-a89b-6d04d06ca368",
   "metadata": {},
   "source": [
    "# SALIENCY\n",
    "\n",
    "Humans, relative to most other species, have distinctly large and visible\n",
    "sclera—the whites of our eyes—and as a result we are uniquely exposed in\n",
    "how we direct our attention, or at the very least, our gaze. Evolutionary\n",
    "biologists have argued, via the “cooperative eye hypothesis,” that this must\n",
    "be a feature, not a bug: that it must point to the fact that cooperation has\n",
    "been uncommonly important in our survival as a species, to the point that\n",
    "the benefits of shared attention outweigh the loss of a certain degree of\n",
    "privacy or discretion.\n",
    "\n",
    "It might be understandable, then, for us to want to expect something\n",
    "similar from our machines: to know not only what they think they see but\n",
    "where, in particular, they are looking. This idea in ML is called \"saliency\". The idea\n",
    "is that if a system is looking at an image and assigning it to some category,\n",
    "then presumably some parts of the image were more important or more\n",
    "influential than others in making that determination. But machine-learning systems can be very unintuitive. Often they\n",
    "latch onto aspects of the training data we did not think were relevant at all,\n",
    "and ignore what we would imagine was the critical information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82081ab3-b21c-4342-b4b9-521b0af26fcc",
   "metadata": {},
   "source": [
    "All of this additional information in a dataset was useless in practice as\n",
    "additional inputs to the model. Learning to predict a patient’s risk of death\n",
    "based on their hospital bill won’t actually help you when a new patient\n",
    "arrives, because of course you don’t know their final bill yet. But rather than\n",
    "serving as additional inputs, this information is useful as additional outputs,\n",
    "additional sources of ground truth in training the model. The technique has\n",
    "come to be known as “multitask learning.”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21455a20-83f8-48ed-99dc-111b3817dd62",
   "metadata": {},
   "source": [
    "### Deconvolution\n",
    "Zeiler and Fergus developed a visualization technique they called\n",
    "“deconvolution,” which was a way to turn intermediate-level activations of\n",
    "the network back into images.\n",
    "\n",
    "The effect was dramatic, insightful. But was it useful? Zeiler popped the\n",
    "hood of the AlexNet model that had won the ImageNet competition in 2012\n",
    "and started digging around, inspecting it using deconvolution. He noticed\n",
    "a bunch of flaws. Some low-level parts of the network had normalized\n",
    "incorrectly, like an overexposed photograph. Other filters had gone “dead”\n",
    "and weren’t detecting anything. Zeiler hypothesized that they weren’t\n",
    "correctly sized for the types of patterns they were trying to match. As\n",
    "astoundingly successful as AlexNet had been, it was carrying some dead\n",
    "weight. It could be improved—and the visualization showed where.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f35a77-91df-41c0-a249-3af9bf36cb9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
