{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0795580f-3b25-42e1-9835-5bed6801085b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BridgeTowerProcessor, BridgeTowerForContrastiveLearning\n",
    "model_name = \"BridgeTower/bridgetower-large-itm-mlm-itc\"\n",
    "processor = BridgeTowerProcessor.from_pretrained(model_name)\n",
    "model = BridgeTowerForContrastiveLearning.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a227e9e-d532-4e96-a8ae-99e2d1514b40",
   "metadata": {},
   "source": [
    "A. we have the image-text pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c89df5-bbdf-4284-8488-53fa0eccb348",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(**processor(\n",
    "        text=\"our specific text is here...\",,\n",
    "        images=Image.open(\"path/to/our/specific/image.png\")\n",
    "        return_tensors=\"pt\",\n",
    "    ))\n",
    "text_embeddding = output.text_embeds.flatten().tolist()\n",
    "image_embeddding = output.image_embeds.flatten().tolist()\n",
    "cross_embeddding = output.cross_embeds.flatten().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86feb02a-8716-4b00-923f-20bfe802207c",
   "metadata": {},
   "source": [
    "B. out of image-text pair, we have only the image part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b815335-938d-4f27-8bba-db0d5cd891a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(**processor(\n",
    "        text=\"captioned image, using LlaVa goes here ...\",\n",
    "        images=Image.open(\"path/to/our/specific/image.png\")\n",
    "        return_tensors=\"pt\",\n",
    "    ))\n",
    "text_embeddding = output.text_embeds.flatten().tolist()\n",
    "image_embeddding = output.image_embeds.flatten().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0faa98a3-5b2d-4aae-9bac-58a0abcc34bc",
   "metadata": {},
   "source": [
    "C. out of image-text pair, we have only the text part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed7ebc4-9e42-44bf-ad75-9a84e3434e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(**processor(\n",
    "        text=\"our specific text goes here...\",\n",
    "        images=Image.open(\"path/to/Any/random/image.png\")\n",
    "        return_tensors=\"pt\",\n",
    "    ))\n",
    "text_embeddding = output.text_embeds.flatten().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e9abba-1aee-4e43-b4d2-71ea234f29e8",
   "metadata": {},
   "source": [
    "D. we need an output when query is a text ::\n",
    "\n",
    "    >>>: 1. create text embedding as above(C.)\n",
    "         2. search text embedding in:\n",
    "             - text embeddings\n",
    "             - cross embeddings\n",
    "         3. rank the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182b61ee-f9f7-4370-8b64-a9a99f73ed17",
   "metadata": {},
   "source": [
    "E. we need an output when query is an image ::\n",
    "\n",
    "    >>>: 1. create image embedding as above(B.)\n",
    "         2. search image embedding in:\n",
    "             - image embeddings\n",
    "             - cross embeddings\n",
    "         3. rank the results\n",
    "    >>>: caption that image using LlaVa and handle the text as in D."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
